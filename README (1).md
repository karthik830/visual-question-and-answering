This repository contains a simple Jupyter notebook that demonstrates a Visual Question Answering (VQA) system using Hugging Face's transformers library. The notebook shows how to load a pretrained vision-language model and use it to answer questions about images. With just a few lines of code, you can input an image and a natural language question, and the model will provide a relevant answer. This example is ideal for those who want to explore the basics of VQA or experiment with multimodal AI models. To run the notebook, you'll need to install the transformers library, and optionally torch, torchvision, and Pillow for full functionality. The implementation is lightweight, easy to follow, and serves as a great starting point for further development in the field of vision-language AI.
